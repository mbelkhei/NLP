{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from math import log\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('spam.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep the text, and class columns\n",
    "data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1,inplace=True)\n",
    "\n",
    "# Rename the two columns\n",
    "data.columns = ['class','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "class      \n",
       "ham    4825\n",
       "spam    747"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into train and test data\n",
    "rows = random.sample(data.index.tolist(), int(data.shape[0]*0.1))\n",
    "df_10 = data.iloc[rows]\n",
    "df_90 = data.drop(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separare the Ham and Spam texts for train and test \n",
    "ham_doc = [i for i in df_90['text'][df_90['class']=='ham'].tolist()]\n",
    "spam_doc = [i for i in df_90['text'][df_90['class']=='spam'].tolist()]\n",
    "all_doc = spam_doc + ham_doc\n",
    "\n",
    "# test \n",
    "spam_test = df_10[df_10['class']=='spam']\n",
    "ham_test  = df_10[df_10['class']=='ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Doc 674\n",
      "Ham Doc 4341\n",
      "All Doc 5015\n"
     ]
    }
   ],
   "source": [
    "print('Spam Doc', len(spam_doc))\n",
    "print('Ham Doc', len(ham_doc))\n",
    "print('All Doc', len(all_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a list of all terms after cleaning all texts \n",
    "terms_in_all = [term.lower() for fx in all_doc for term in fx.split() if term.lower().isalpha() and term.lower() not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of terms in all files : 32563\n",
      "Size of vocab : 5688\n"
     ]
    }
   ],
   "source": [
    "# Frequency county of terms\n",
    "vocab = Counter(terms_in_all)\n",
    "print (\"Total number of terms in all files :\", len(terms_in_all))\n",
    "print (\"Size of vocab :\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms in spam docs : 6911\n",
      "Spam vocab : 1391\n",
      "Terms in ham docs : 25652\n",
      "Ham vocab : 5022\n"
     ]
    }
   ],
   "source": [
    "terms_in_spam = [term.lower() for fx in spam_doc for term in fx.split() if term.lower().isalpha() and term.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "counts_spam = Counter(terms_in_spam)\n",
    "print (\"Terms in spam docs :\", len(terms_in_spam))\n",
    "print (\"Spam vocab :\", len(counts_spam))\n",
    "\n",
    "terms_in_ham = [term.lower() for fx in ham_doc for term in fx.split() if term.lower().isalpha() and term.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "counts_ham = Counter(terms_in_ham)\n",
    "print (\"Terms in ham docs :\", len(terms_in_ham))\n",
    "print (\"Ham vocab :\", len(counts_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam prior probability : 0.13\n",
      "Ham prior probability : 0.87\n"
     ]
    }
   ],
   "source": [
    "# calculate probabilities of Spam and Ham\n",
    "P_spam = len(spam_doc)/float(len(all_doc))\n",
    "print (\"Spam prior probability : {:.2f}\".format(P_spam))\n",
    "P_ham = len(ham_doc)/float(len(all_doc))\n",
    "print (\"Ham prior probability : {:.2f}\".format(P_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate conditional probability\n",
    "# Log of probabilities is taken to prevent underflow (extremely small prob. values)\n",
    "#Laplace smoothing (counts+1) is done to prevent multiplication by zero\n",
    "cond_prob = {'spam': {}, 'ham': {}}\n",
    "score_spam = log(P_spam)\n",
    "score_ham = log(P_ham)\n",
    "for term in vocab:\n",
    "    term_spam_count = counts_spam[term] \n",
    "    term_ham_count = counts_ham[term]\n",
    "    cond_prob['spam'][term] = (term_spam_count+1)/float(len(terms_in_spam)+len(vocab)) \n",
    "    cond_prob['ham'][term] = (term_ham_count+1)/float(len(terms_in_ham)+len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SpamClassifier:\n",
    "    def __init__(self):\n",
    "        self.prior_spam = None\n",
    "        self.prior_ham = None\n",
    "        self.likelihood = None\n",
    "        \n",
    "    def classify(self, message_terms):\n",
    "        score_spam = self.prior_spam\n",
    "        score_ham = self.prior_ham\n",
    "        for term in message_terms:\n",
    "            try:\n",
    "                score_spam += log(self.likelihood['spam'][term]) \n",
    "            except KeyError as e:\n",
    "                score_spam += log(1/float(len(terms_in_spam)+len(vocab)))\n",
    "            try:\n",
    "                score_ham += log(self.likelihood['ham'][term])\n",
    "            except KeyError as e:\n",
    "                score_ham += log(1/float(len(terms_in_ham)+len(vocab)))\n",
    "        if score_spam > score_ham:\n",
    "            return 1 #SPAM\n",
    "        else:\n",
    "            return 0 #HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiliaze the classifier\n",
    "clf = SpamClassifier()\n",
    "clf.prior_spam = log(P_spam) \n",
    "clf.prior_ham = log(P_ham)\n",
    "clf.likelihood = cond_prob\n",
    "n_spam = spam_test.shape[0]\n",
    "n_ham = ham_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positives : 73  Number of Negatives  : 484\n",
      "True Positives (TP) : 68  False Positives (FP) : 15\n",
      "True Negatives (TN) : 469  False Negatives (FN) : 5\n",
      "True Positive Rate (TPR) : 0.932  False Positive Rate (FPR) : 0.031\n"
     ]
    }
   ],
   "source": [
    "TP=0\n",
    "FP=0\n",
    "TN=0\n",
    "FN=0\n",
    "# Make predictions\n",
    "for row in df_10.itertuples():\n",
    "    label = row[1]\n",
    "    text  = [term.lower() for term in row[2].split()  if term.lower().isalpha() and term.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "    result = clf.classify(text)\n",
    "    if label == 'spam' and result == 1:   #file spam and classified spam\n",
    "        TP += 1\n",
    "    elif label == 'ham' and result == 1:  #file ham but classifed spam\n",
    "        FP += 1\n",
    "    elif label == 'spam' and result == 0: #file spam but classifed ham\n",
    "        FN += 1                 \n",
    "    elif label == 'ham' and result ==0:   #file ham and classified ham\n",
    "        TN += 1\n",
    "\n",
    "Total = TP+TN+FP+FN\n",
    "TPR = TP/float(n_spam) #sensitivity\n",
    "FPR = FP/float(n_ham) # 1-specificity            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print (\"Number of Positives :\", n_spam, \" Number of Negatives  :\", n_ham)\n",
    "print (\"True Positives (TP) :\", TP, \" False Positives (FP) :\", FP)\n",
    "print (\"True Negatives (TN) :\", TN, \" False Negatives (FN) :\", FN)\n",
    "print (\"True Positive Rate (TPR) :\", round(TPR, 3), \" False Positive Rate (FPR) :\", round(FPR, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Metrics : \n",
      "Accuracy  : 0.964    Error Rate : 0.036\n",
      "Precision : 0.819    Recall     : 0.932\n",
      "F1-score  : 0.872\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy, Precision, Recall, and F1-score\n",
    "accuracy = (TP+TN)/float(Total)\n",
    "precision =  TP/float(TP+FP)\n",
    "recall = TP/float(TP+FN)\n",
    "f1_score = (2*(precision*recall))/float(precision+recall)\n",
    "error_rate = (FP+FN)/float(Total)\n",
    "print (\"Classifier Metrics : \")\n",
    "print (\"Accuracy  :\", round(accuracy, 3), \"   Error Rate :\", round(error_rate, 3))\n",
    "print (\"Precision :\", round(precision, 3), \"   Recall     :\", round(recall, 3))\n",
    "print (\"F1-score  :\", round(f1_score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
