{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Index:\n",
    "    \"\"\" Inverted index datastructure \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, stemmer=None, stopwords=None):\n",
    "        \"\"\"\n",
    "        tokenizer -- NLTK compatible tokenizer function\n",
    "        stemmer   -- NLTK compatible stemmer\n",
    "        stopwords -- list of ignored words\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer   = stemmer\n",
    "        self.index     = defaultdict(list)\n",
    "        self.documents = {}\n",
    "        self.__unique_id = 0\n",
    "        \n",
    "        if not stopwords:\n",
    "            self.stopwords = set()\n",
    "        else:\n",
    "            self.stopwords = set(stopwords)\n",
    "        \n",
    "    def lookup(self, word):\n",
    "        \"\"\"\n",
    "        Lookup a word in the index\n",
    "        \"\"\"\n",
    "        word = word.lower()\n",
    "        if self.stemmer:\n",
    "            word = self.stemmer.stem(word)\n",
    "        return [self.documents.get(id, None) for id in self.index.get(word)]\n",
    "    def add(self, document):\n",
    "        \"\"\"\n",
    "        Add a document string to the index\n",
    "        \"\"\"\n",
    "        for token in [t.lower() for t in nltk.word_tokenize(document)]:\n",
    "            if token in self.stopwords:\n",
    "                continue\n",
    "            if self.stemmer:\n",
    "                token = self.stemmer.stem(token)\n",
    "            if self.__unique_id not in self.index[token]:\n",
    "                self.index[token].append(self.__unique_id)\n",
    "        self.documents[self.__unique_id] = document\n",
    "        self.__unique_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = Index(nltk.word_tokenize, EnglishStemmer(),nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# TOP10 Dire straits\n",
    "index.add('Industrial Disease')\n",
    "index.add('Private Investigations')\n",
    "index.add('So Far Away')\n",
    "index.add('Twisting by the Pool')\n",
    "index.add('Skateaway')\n",
    "index.add('Walk of Life')\n",
    "index.add('Romeo and Juliet')\n",
    "index.add('Tunnel of Love')\n",
    "index.add('Money for Nothing')\n",
    "index.add('Sultans of Swing')\n",
    " \n",
    " \n",
    "# TOP10 Led Zeppelin\n",
    "index.add('Stairway To Heaven')\n",
    "index.add('Kashmir')\n",
    "index.add('Achilles Last Stand')\n",
    "index.add('Whole Lotta Love')\n",
    "index.add('Immigrant Song')\n",
    "index.add('Black Dog')\n",
    "index.add('When The Levee Breaks')\n",
    "index.add('Since I\\'ve Been Lovin\\' You')\n",
    "index.add('Since I\\'ve Been Loving You')\n",
    "index.add('Over the Hills and Far Away')\n",
    "index.add('Dazed and Confused') \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tunnel of Love', 'Whole Lotta Love', \"Since I've Been Loving You\"]\n"
     ]
    }
   ],
   "source": [
    "# Make querries:\n",
    "print(index.lookup('loves'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tunnel of Love', 'Whole Lotta Love', \"Since I've Been Loving You\"]\n"
     ]
    }
   ],
   "source": [
    "print (index.lookup('loved'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dazed and Confused']\n"
     ]
    }
   ],
   "source": [
    "print (index.lookup('daze'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dazed and Confused']\n"
     ]
    }
   ],
   "source": [
    "print (index.lookup('confusion'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
